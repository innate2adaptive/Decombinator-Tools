# How to run Decombinator jobs on the UCL CS HPC cluster

*This guide is only relevant for members of the Innate2Adaptive group at UCL. At present it is designed to work with batches that have samples generated by a single protocol and species (e.g. RACE on Humans). If you are from another group utilising the COLCC resource, feel free to adapt the scripts found here. If you would like help with this, raise an issue in this repository and we will try to assist.*

1. You will need access to the UCL CS HPC cluster which can be requested [here](https://hpc.cs.ucl.ac.uk/account-form/).
2. You will need to be added to the `inn2adap` user group to get access to our project storage. Ask for another member of the group to give you the directory path and request access for you when you obtain your login details.
    - In order to `cd` to the project storage you will need to directly `cd` to the project, due to dynamic mounting, the directory will not appear in `ls` or a directory GUI.
    - For convenience, create a bash script in your home directory with a `cd` command to the project directory.
3. Create a new batch directory in the project directory and place your `.tar` file there, make sure the `.tar` file is also backed up to the RDS as the project directory is not itself backed up.
    - See the UCL CS HPC [website](https://hpc.cs.ucl.ac.uk/ssh-scp/) for information on how to set up port-forwarding if you need to `scp` data to the cluster.
4. Copy the contents of `Decombinator-Tools/jobs/cshpc/` to your batch directory.
    - There are 4 scripts: `submit.sh` which you will call to unpack your `.tar` file and submit all of your samples as jobs, `dcr_job.qsub.sh` is the job script which instructs the cluster how to treat your task, `resubmit.sh` is a handy script that you can use to resubmit any jobs which fail due to various reasons (see the troubleshooting section below), and `repack.sh` which will format your processed data into the Chain lab standard storage format.
5. Check `dcr_job.qsub.sh` to make sure it has the correct pipeline settings for your data.
    - If you have samples processed by different protocols in your batch, you will have to split them manually and edit `submit.sh` to skip the already completed steps.
6. Run `sh submit.sh` to submit all samples in the `.tar` file to the job scheduler.
7. Check if your jobs begin running successfully with `qstat`.
8. TROUBLESHOOTING: If a subset of jobs fails to complete, it may be due to memory or runtime limits. Investigate why using `summarise.sh` script. More detailed investigation can be performed using the `.o` files in each job subdirectory (feel free to ask for help with this). Edit the copy of `dcr_job.qsub.sh` located in the top level of your batch directory, and then run `sh resubmit.sh`. This will resubmit all jobs that failed to complete, but using your new memory or runtime limits.
    - If your `.o` file ends with `MemoryError`, increase the memory requested in `dcr_job.qsub.sh`.
    - If your `.o` ends without any error message, the job likely hit the runtime limit. Increase the runtime requested in `dcr_job.qsub.sh`.
    - For other errors, inspect the error stack message carefully, as often it will be related to naming convention issues. Feel free to open an issue on this repository for help.
9. Create a `.csv` with your batch name and the order in which you would like your samples presented in the summary sheet.
10. Once all jobs are complete, check you have all the results you expected with `find temp/ -type f -name "*tsv*" | wc -l` (remove `| wc -l` to see filenames rather than counts), and then run `sh summarise.sh` to package the results for transfer to the RDS.
11. When you have verified that the data is safely saved onto the RDS, delete the batch directory containing the analysis on the CS HPC cluster. Only you have the permissions to delete this folder and we have a 500GB limit in the project directory.
