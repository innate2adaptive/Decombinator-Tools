# How to run Decombinator jobs on the UCL CS HPC cluster

*This guide is only relevant for members of the Innate2Adaptive group at UCL. At present it is designed to work with batches that have samples generated by a single protocol and species (e.g. RACE on Humans).*

1. You will need access to the UCL CS HPC cluster which can be requested [here](https://hpc.cs.ucl.ac.uk/account-form/).
2. You will need to be added to the `inn2adap` user group to get access to our project storage. Ask for another member of the group to give you the directory path and request access for you when you obtain your login details.
    - In order to `cd` to the project storage you will need to directly `cd` to the project, due to dynamic mounting, the directory will not appear in `ls` or a directory GUI.
    - For convenience, create a bash script in your home directory with a `cd` command to the project directory.
3. Create a new batch directory in the project directory and place your `.tar` file there, make sure the `.tar` file is also backed up to the RDS as the project directory is not itself backed up.
    - See the UCL CS HPC [website](https://hpc.cs.ucl.ac.uk/ssh-scp/) for information on how to set up port-forwarding if you need to `scp` data to the cluster.
4. Copy the contents of `Decombinator-Tools/jobs/cshpc/` to your batch directory.
5. Check `dcr_job.qsub.sh` to make sure it has the correct pipeline settings for your data.
    - If you have samples processed by different protocols in your batch, you will have to split them manually and edit `submit.sh` to skip the already completed steps.
6. Run `source unpack_and_submit.sh` to submit all samples in the `.tar` file to the job scheduler.
7. Check if your jobs begin running successfully with `qstat`.
8. If a subset of jobs fails to complete, it may be due to memory or runtime limits. Investigate why using the `.o` files in each job subdirectory (feel free to ask for help with this), edit the copy of `dcr_job.qsub.sh` located in the top level of your batch directory, and then run `source resubmit.sh`. This will resubmit all jobs that failed to complete, but using your new memory or runtime limits.
9. Create a `.csv` with your batch name and the order in which you would like your samples presented in the summary sheet.
10. Once all jobs are complete, run `source summarise.sh` to package the results for transfer to the RDS.
11. When you have verified that the data is safely saved onto the RDS, delete the batch directory containing the analysis on the CS HPC cluster. Only you have the permissions to delete this folder and we have a 500GB limit in the project directory.
